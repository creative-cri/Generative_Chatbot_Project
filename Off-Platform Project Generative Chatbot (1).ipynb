{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdac50fb",
   "metadata": {},
   "source": [
    "# Generative Chatbot Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a523c9d",
   "metadata": {},
   "source": [
    "# twiter_prep.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db884079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('im changing my cats name to tofu', 'imagine naming ur cat gooby instead of tofu to begin w..'), (\"i wish she'd learn to use cutlery, she's making a right mess!\", 'omg... just like my cat!'), ('can you find the cat in each photo?üòÇ', 'what cat?'), ('glad we agree', \"i don't like the way that cat is looking at me\"), ('it shen , he meows and purrs lot !', 'good cat'), ('i guess someone had a busy day', 'cats are very busy creatures, always on important cat business'), ('no, his name is . . . mr. tiggles üòÖ', 'that mr mayhem commercial where he acts like a cat knocking that stuff off the counter makes me laugh.'), ('‚îÄ were always there when it came to cats. brushing her knuckles across its cheek.', 'isn‚Äôt he lovely? [magnus smiled softly at the sight of the cat getting familiar, purring'), ('casual photo of me and max getting ready to judge your replies to this tweet..', 'i love your cat ryan!'), ('the cats in cats as presidential nominees: a thread (1/?)', 'biden is this cat:'), ('she nods a bit before padding over to him. food and water sound amazing right now. and somewhere warm to nap.', 'tenya poured in some fresh water, and some cat food, then slid the bowls over to the cat.'), ('omg i love lil tuxedo babies thats what rae isüò≠üíòüíò', 'peaches (our 20 year old geriatric) had a tuxedo brother named gizmo'), ('update: he jumped for real this morning. thankfully, he landed on a cardboard box and seems to have walked away', 'stunt cat. :/'), ('larry.... üòªüòªüòª they should install a buzzer for you to be able to paw for entry', 'one of the panels should be refitted as a cat flap'), ('i think i may have found what tim was talking about', 'that cat looks thrilled about the whole thing'), ('i am eating hummus on rye. hummus. on. rye. and he still wants my sandwich. #tongueouttuesday', 'that is the face of a cat who never, ever gets fed. üòÇ'), ('might as well put it to good use!', 'absolutely. laptop on bed, cat on my feet, very cozy. üòÖ'), ('hey folks! pepita is awake &amp; i am picking her up later this afternoon! i accidentally stepped on her tail last night', 'i‚Äôm so glad she‚Äôs okay! but also literally everyone who has a cat has stepped on it.'), (\"appreciate your pets everyone. they won't be there forever. let's see your pets of past and present\", 'this is milo celebrating my arrival back after xmas doing the you haven‚Äôt been with another cat check'), ('(he got distracted too. there are so many! and all of them are so cute! arthur possibly is in heaven', \"what would you like to order? (he's stroking that cat and it's purring, settling on his lap. feels nice‚Ä¶\"), ('my boss and i have been working on this one case for an hour and 7 mins üåö', 'my cat acted a fool the whole time sis was literally trying to open the fridge üò≠'), ('help her‚òùÔ∏è', 'my my cat mossad agent demand it!'), ('absolutely. laptop on bed, cat on my feet, very cozy. üòÖ', 'same minus the cat üòÇ bc i didn‚Äôt leave my apt during this episode bc they told me i didn‚Äôt need to'), ('having a really rough time today friends. please send pictures of cats. big hugs xox', 'here‚Äôs a cat that meowed at my front door so i let it in'), (\"meet the fat, bald cat that's becoming a body positivity icon\", 'this isnt good, that cat is most likely suffering because it cant just be a normal cat.'), (\"they'll be left feline sad üòø\", 'the bbc is a cat-astrophe')]\n"
     ]
    }
   ],
   "source": [
    "from itertools import zip_longest\n",
    "import re\n",
    "\n",
    "data_path = \"cat.txt\"\n",
    "\n",
    "# Defining lines as a list of each line\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "  lines = f.read().split('\\n')\n",
    "\n",
    "lines = [re.sub(r\"(?:\\@|https?\\://)\\S+\", \"\", line).strip() for line in lines]\n",
    "\n",
    "# group lines by response pair\n",
    "\n",
    "def grouper(iterable, n, fillvalue=None):\n",
    "    args = [iter(iterable)] * n\n",
    "    return zip_longest(*args, fillvalue=fillvalue)\n",
    "pairs = list(grouper(lines, 2))\n",
    "print(pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a232e2a",
   "metadata": {},
   "source": [
    "# preprocessing1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae64a91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "#from twitter_prep import pairs\n",
    "\n",
    "# Building empty lists to hold sentences\n",
    "input_docs = []\n",
    "target_docs = []\n",
    "# Building empty vocabulary sets\n",
    "input_tokens = set()\n",
    "target_tokens = set()\n",
    "\n",
    "for line in pairs[:15]:\n",
    "  # Input and target sentences are separated by tabs\n",
    "  input_doc, target_doc = line[0], line[1]\n",
    "  # Appending each input sentence to input_docs\n",
    "  input_docs.append(input_doc)\n",
    "  # Splitting words from punctuation  \n",
    "  target_doc = \" \".join(re.findall(r\"[\\w']+|[^\\s\\w]\", target_doc))\n",
    "  # Redefine target_doc below \n",
    "  # and append it to target_docs:\n",
    "  target_doc = '<START> ' + target_doc + ' <END>'\n",
    "  target_docs.append(target_doc)\n",
    "  \n",
    "  # Now we split up each sentence into words\n",
    "  # and add each unique word to our vocabulary set\n",
    "  for token in re.findall(r\"[\\w']+|[^\\s\\w]\", input_doc):\n",
    "    # Add your code here:\n",
    "    if token not in input_tokens:\n",
    "      input_tokens.add(token)\n",
    "  for token in target_doc.split():\n",
    "    # And here:\n",
    "    if token not in target_tokens:\n",
    "      target_tokens.add(token)\n",
    "\n",
    "input_tokens = sorted(list(input_tokens))\n",
    "target_tokens = sorted(list(target_tokens))\n",
    "\n",
    "# Create num_encoder_tokens and num_decoder_tokens:\n",
    "num_encoder_tokens = len(input_tokens)\n",
    "num_decoder_tokens = len(target_tokens)\n",
    "\n",
    "max_encoder_seq_length = max([len(re.findall(r\"[\\w']+|[^\\s\\w]\", input_doc)) for input_doc in input_docs])\n",
    "max_decoder_seq_length = max([len(re.findall(r\"[\\w']+|[^\\s\\w]\", target_doc)) for target_doc in target_docs])\n",
    "\n",
    "input_features_dict = dict(\n",
    "    [(token, i) for i, token in enumerate(input_tokens)])\n",
    "target_features_dict = dict(\n",
    "    [(token, i) for i, token in enumerate(target_tokens)])\n",
    "\n",
    "reverse_input_features_dict = dict(\n",
    "    (i, token) for token, i in input_features_dict.items())\n",
    "reverse_target_features_dict = dict(\n",
    "    (i, token) for token, i in target_features_dict.items())\n",
    "\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_docs), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_docs), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_docs), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "\n",
    "for line, (input_doc, target_doc) in enumerate(zip(input_docs, target_docs)):\n",
    "\n",
    "  for timestep, token in enumerate(re.findall(r\"[\\w']+|[^\\s\\w]\", input_doc)):\n",
    "    # Assign 1. for the current line, timestep, & word\n",
    "    # in encoder_input_data:\n",
    "    encoder_input_data[line, timestep, input_features_dict[token]] = 1.\n",
    "    # add in conditional for handling unknown tokens (when token is not in input features dict)\n",
    "\n",
    "  for timestep, token in enumerate(target_doc.split()):\n",
    "\n",
    "    decoder_input_data[line, timestep, target_features_dict[token]] = 1.\n",
    "    if timestep > 0:\n",
    "\n",
    "      decoder_target_data[line, timestep - 1, target_features_dict[token]] = 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4aa737",
   "metadata": {},
   "source": [
    "# training_model1.py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c967f1eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1/1 [==============================] - 6s 6s/step - loss: 2.1368 - accuracy: 0.0000e+00 - val_loss: 1.5650 - val_accuracy: 0.0370\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.1181 - accuracy: 0.0864 - val_loss: 1.5585 - val_accuracy: 0.0617\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.0941 - accuracy: 0.0710 - val_loss: 1.4950 - val_accuracy: 0.0370\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 1.9581 - accuracy: 0.0370 - val_loss: 1.5576 - val_accuracy: 0.0617\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.0685 - accuracy: 0.0864 - val_loss: 1.5289 - val_accuracy: 0.0617\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 1.9677 - accuracy: 0.0494 - val_loss: 1.5013 - val_accuracy: 0.0617\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 1.9005 - accuracy: 0.0463 - val_loss: 1.5201 - val_accuracy: 0.0370\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 1.8566 - accuracy: 0.0586 - val_loss: 1.5239 - val_accuracy: 0.0617\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 1.8703 - accuracy: 0.0494 - val_loss: 1.5466 - val_accuracy: 0.0370\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 1.8340 - accuracy: 0.0370 - val_loss: 1.5447 - val_accuracy: 0.0494\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 1.8501 - accuracy: 0.0648 - val_loss: 1.5246 - val_accuracy: 0.0370\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 1.8393 - accuracy: 0.0401 - val_loss: 1.5661 - val_accuracy: 0.0494\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 1.8670 - accuracy: 0.0679 - val_loss: 1.5773 - val_accuracy: 0.0370\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 1.8078 - accuracy: 0.0432 - val_loss: 1.5669 - val_accuracy: 0.0370\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 1.7938 - accuracy: 0.0432 - val_loss: 1.5803 - val_accuracy: 0.0494\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 1.7611 - accuracy: 0.0586 - val_loss: 1.5724 - val_accuracy: 0.0370\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 1.7542 - accuracy: 0.0463 - val_loss: 1.6037 - val_accuracy: 0.0247\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 1.7550 - accuracy: 0.0556 - val_loss: 1.5585 - val_accuracy: 0.0370\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 1.8478 - accuracy: 0.0463 - val_loss: 1.5659 - val_accuracy: 0.0370\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 1.7907 - accuracy: 0.0494 - val_loss: 1.5719 - val_accuracy: 0.0494\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 1.7607 - accuracy: 0.0432 - val_loss: 1.5703 - val_accuracy: 0.0370\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 1.7438 - accuracy: 0.0463 - val_loss: 1.5846 - val_accuracy: 0.0370\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 1.7229 - accuracy: 0.0525 - val_loss: 1.5639 - val_accuracy: 0.0370\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 1.7340 - accuracy: 0.0494 - val_loss: 1.5995 - val_accuracy: 0.0370\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 1.6913 - accuracy: 0.0494 - val_loss: 1.5578 - val_accuracy: 0.0370\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 1.7462 - accuracy: 0.0494 - val_loss: 1.5809 - val_accuracy: 0.0370\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 1.6761 - accuracy: 0.0525 - val_loss: 1.5806 - val_accuracy: 0.0370\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 1.6576 - accuracy: 0.0525 - val_loss: 1.5930 - val_accuracy: 0.0494\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 1.6673 - accuracy: 0.0586 - val_loss: 1.5830 - val_accuracy: 0.0370\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 1.7227 - accuracy: 0.0586 - val_loss: 1.5701 - val_accuracy: 0.0370\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 1.6608 - accuracy: 0.0648 - val_loss: 1.6002 - val_accuracy: 0.0494\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 1.6436 - accuracy: 0.0525 - val_loss: 1.5690 - val_accuracy: 0.0370\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 1.6470 - accuracy: 0.0617 - val_loss: 1.5989 - val_accuracy: 0.0370\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 1.6196 - accuracy: 0.0556 - val_loss: 1.5827 - val_accuracy: 0.0370\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 1.6291 - accuracy: 0.0741 - val_loss: 1.5831 - val_accuracy: 0.0370\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 1.6522 - accuracy: 0.0648 - val_loss: 1.6147 - val_accuracy: 0.0247\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 1.5859 - accuracy: 0.0617 - val_loss: 1.5672 - val_accuracy: 0.0370\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 1.6400 - accuracy: 0.0586 - val_loss: 1.5975 - val_accuracy: 0.0247\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 1.6022 - accuracy: 0.0772 - val_loss: 1.5968 - val_accuracy: 0.0370\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 1.7576 - accuracy: 0.0556 - val_loss: 1.5658 - val_accuracy: 0.0370\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 1.6672 - accuracy: 0.0648 - val_loss: 1.5739 - val_accuracy: 0.0370\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 1.6134 - accuracy: 0.0648 - val_loss: 1.5794 - val_accuracy: 0.0370\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 1.5777 - accuracy: 0.0772 - val_loss: 1.6051 - val_accuracy: 0.0494\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 1.5843 - accuracy: 0.0741 - val_loss: 1.5951 - val_accuracy: 0.0247\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.5522 - accuracy: 0.0802 - val_loss: 1.6289 - val_accuracy: 0.0247\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 1.6247 - accuracy: 0.0617 - val_loss: 1.6086 - val_accuracy: 0.0494\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 1.6150 - accuracy: 0.0772 - val_loss: 1.5730 - val_accuracy: 0.0370\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.5875 - accuracy: 0.0833 - val_loss: 1.6234 - val_accuracy: 0.0123\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 1.5332 - accuracy: 0.0833 - val_loss: 1.6149 - val_accuracy: 0.0494\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 1.6128 - accuracy: 0.0833 - val_loss: 1.5919 - val_accuracy: 0.0370\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 1.5393 - accuracy: 0.0957 - val_loss: 1.6001 - val_accuracy: 0.0370\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 1.4910 - accuracy: 0.0772 - val_loss: 1.5955 - val_accuracy: 0.0123\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.4680 - accuracy: 0.1080 - val_loss: 1.6180 - val_accuracy: 0.0123\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 1.4488 - accuracy: 0.0988 - val_loss: 1.5844 - val_accuracy: 0.0370\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 1.5856 - accuracy: 0.0864 - val_loss: 1.6060 - val_accuracy: 0.0370\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 1.5159 - accuracy: 0.0895 - val_loss: 1.5905 - val_accuracy: 0.0370\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 1.4709 - accuracy: 0.0895 - val_loss: 1.5989 - val_accuracy: 0.0247\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 1.4374 - accuracy: 0.1080 - val_loss: 1.5776 - val_accuracy: 0.0247\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 1.4163 - accuracy: 0.1173 - val_loss: 1.6146 - val_accuracy: 0.0123\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 1.3975 - accuracy: 0.1080 - val_loss: 1.5870 - val_accuracy: 0.0247\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 1.4271 - accuracy: 0.1019 - val_loss: 1.6387 - val_accuracy: 0.0123\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 1.4526 - accuracy: 0.0864 - val_loss: 1.6030 - val_accuracy: 0.0370\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.5190 - accuracy: 0.1080 - val_loss: 1.6540 - val_accuracy: 0.0370\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 1.4146 - accuracy: 0.0957 - val_loss: 1.5963 - val_accuracy: 0.0123\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 1.3850 - accuracy: 0.1173 - val_loss: 1.6319 - val_accuracy: 0.0123\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 1.3431 - accuracy: 0.1173 - val_loss: 1.5838 - val_accuracy: 0.0123\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 1.3919 - accuracy: 0.1389 - val_loss: 1.6290 - val_accuracy: 0.0247\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 1.3203 - accuracy: 0.1142 - val_loss: 1.6050 - val_accuracy: 0.0370\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 1.3821 - accuracy: 0.1142 - val_loss: 1.6678 - val_accuracy: 0.0247\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.3275 - accuracy: 0.1111 - val_loss: 1.5901 - val_accuracy: 0.0370\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 1.4644 - accuracy: 0.1296 - val_loss: 1.6009 - val_accuracy: 0.0247\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.3948 - accuracy: 0.1235 - val_loss: 1.6146 - val_accuracy: 0.0370\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.3506 - accuracy: 0.1296 - val_loss: 1.6225 - val_accuracy: 0.0370\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 1.2940 - accuracy: 0.1389 - val_loss: 1.6536 - val_accuracy: 0.0247\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 1.3526 - accuracy: 0.1265 - val_loss: 1.6053 - val_accuracy: 0.0370\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 1.5228 - accuracy: 0.0741 - val_loss: 1.6236 - val_accuracy: 0.0247\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.3820 - accuracy: 0.1235 - val_loss: 1.6376 - val_accuracy: 0.0370\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 1.2951 - accuracy: 0.1389 - val_loss: 1.6270 - val_accuracy: 0.0247\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 1.2741 - accuracy: 0.1543 - val_loss: 1.6631 - val_accuracy: 0.0370\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 1.2471 - accuracy: 0.1451 - val_loss: 1.6396 - val_accuracy: 0.0247\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 1.2822 - accuracy: 0.1420 - val_loss: 1.6494 - val_accuracy: 0.0247\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 1.2625 - accuracy: 0.1451 - val_loss: 1.6422 - val_accuracy: 0.0247\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 1.2526 - accuracy: 0.1790 - val_loss: 1.6697 - val_accuracy: 0.0247\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.2109 - accuracy: 0.1420 - val_loss: 1.5964 - val_accuracy: 0.0370\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 1.3280 - accuracy: 0.1389 - val_loss: 1.6601 - val_accuracy: 0.0370\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 1.1723 - accuracy: 0.1698 - val_loss: 1.6470 - val_accuracy: 0.0247\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 1.2506 - accuracy: 0.1543 - val_loss: 1.6450 - val_accuracy: 0.0370\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 1.2309 - accuracy: 0.1296 - val_loss: 1.5982 - val_accuracy: 0.0247\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.3718 - accuracy: 0.1173 - val_loss: 1.6566 - val_accuracy: 0.0247\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.2268 - accuracy: 0.1667 - val_loss: 1.6749 - val_accuracy: 0.0247\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.1755 - accuracy: 0.1759 - val_loss: 1.6379 - val_accuracy: 0.0370\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.1534 - accuracy: 0.1914 - val_loss: 1.6703 - val_accuracy: 0.0247\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 1.1907 - accuracy: 0.1420 - val_loss: 1.6548 - val_accuracy: 0.0370\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 1.2271 - accuracy: 0.1605 - val_loss: 1.6649 - val_accuracy: 0.0247\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 1.1666 - accuracy: 0.1574 - val_loss: 1.6649 - val_accuracy: 0.0247\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 1.1283 - accuracy: 0.1975 - val_loss: 1.6306 - val_accuracy: 0.0370\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 1.3588 - accuracy: 0.1265 - val_loss: 1.6572 - val_accuracy: 0.0247\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 1.1330 - accuracy: 0.1975 - val_loss: 1.6322 - val_accuracy: 0.0247\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 1.1212 - accuracy: 0.1975 - val_loss: 1.7009 - val_accuracy: 0.0370\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 1.0665 - accuracy: 0.2099 - val_loss: 1.6273 - val_accuracy: 0.0370\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 1.0672 - accuracy: 0.2191 - val_loss: 1.7141 - val_accuracy: 0.0247\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 1.0623 - accuracy: 0.1790 - val_loss: 1.6401 - val_accuracy: 0.0370\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 1.1595 - accuracy: 0.1698 - val_loss: 1.6793 - val_accuracy: 0.0370\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 1.0648 - accuracy: 0.1759 - val_loss: 1.6456 - val_accuracy: 0.0247\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 1.1071 - accuracy: 0.1944 - val_loss: 1.7164 - val_accuracy: 0.0247\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 1.0470 - accuracy: 0.1821 - val_loss: 1.6224 - val_accuracy: 0.0247\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 1.1668 - accuracy: 0.1698 - val_loss: 1.6988 - val_accuracy: 0.0123\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 1.0318 - accuracy: 0.1914 - val_loss: 1.6883 - val_accuracy: 0.0247\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 1.0565 - accuracy: 0.2346 - val_loss: 1.6898 - val_accuracy: 0.0123\n",
      "Epoch 110/200\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 1.0030 - accuracy: 0.1914 - val_loss: 1.6632 - val_accuracy: 0.0247\n",
      "Epoch 111/200\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 1.0845 - accuracy: 0.1975 - val_loss: 1.7171 - val_accuracy: 0.0370\n",
      "Epoch 112/200\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.9966 - accuracy: 0.1852 - val_loss: 1.6312 - val_accuracy: 0.0247\n",
      "Epoch 113/200\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 1.0311 - accuracy: 0.2160 - val_loss: 1.7335 - val_accuracy: 0.0247\n",
      "Epoch 114/200\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.9769 - accuracy: 0.1944 - val_loss: 1.6621 - val_accuracy: 0.0247\n",
      "Epoch 115/200\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.0804 - accuracy: 0.2006 - val_loss: 1.7334 - val_accuracy: 0.0247\n",
      "Epoch 116/200\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.1050 - accuracy: 0.1512 - val_loss: 1.7082 - val_accuracy: 0.0247\n",
      "Epoch 117/200\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.0929 - accuracy: 0.2099 - val_loss: 1.6368 - val_accuracy: 0.0247\n",
      "Epoch 118/200\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.1302 - accuracy: 0.1852 - val_loss: 1.6973 - val_accuracy: 0.0247\n",
      "Epoch 119/200\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 1.0307 - accuracy: 0.2099 - val_loss: 1.6326 - val_accuracy: 0.0370\n",
      "Epoch 120/200\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.9870 - accuracy: 0.2130 - val_loss: 1.6067 - val_accuracy: 0.0370\n",
      "Epoch 121/200\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 1.0141 - accuracy: 0.1944 - val_loss: 1.6692 - val_accuracy: 0.0247\n",
      "Epoch 122/200\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.0884 - accuracy: 0.1728 - val_loss: 1.6594 - val_accuracy: 0.0247\n",
      "Epoch 123/200\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 1.2142 - accuracy: 0.1698 - val_loss: 1.6438 - val_accuracy: 0.0123\n",
      "Epoch 124/200\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 1.0595 - accuracy: 0.2037 - val_loss: 1.7461 - val_accuracy: 0.0247\n",
      "Epoch 125/200\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.9931 - accuracy: 0.2191 - val_loss: 1.6350 - val_accuracy: 0.0247\n",
      "Epoch 126/200\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.9405 - accuracy: 0.2438 - val_loss: 1.7370 - val_accuracy: 0.0370\n",
      "Epoch 127/200\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 1.0085 - accuracy: 0.2222 - val_loss: 1.6252 - val_accuracy: 0.0247\n",
      "Epoch 128/200\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.9348 - accuracy: 0.2284 - val_loss: 1.6605 - val_accuracy: 0.0370\n",
      "Epoch 129/200\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.9320 - accuracy: 0.2315 - val_loss: 1.6562 - val_accuracy: 0.0247\n",
      "Epoch 130/200\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.9088 - accuracy: 0.2377 - val_loss: 1.6756 - val_accuracy: 0.0370\n",
      "Epoch 131/200\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.9351 - accuracy: 0.2284 - val_loss: 1.6479 - val_accuracy: 0.0247\n",
      "Epoch 132/200\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.9170 - accuracy: 0.2623 - val_loss: 1.7031 - val_accuracy: 0.0370\n",
      "Epoch 133/200\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.9046 - accuracy: 0.2346 - val_loss: 1.6209 - val_accuracy: 0.0247\n",
      "Epoch 134/200\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.9842 - accuracy: 0.2253 - val_loss: 1.6903 - val_accuracy: 0.0123\n",
      "Epoch 135/200\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.9128 - accuracy: 0.2284 - val_loss: 1.6670 - val_accuracy: 0.0247\n",
      "Epoch 136/200\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.9500 - accuracy: 0.2531 - val_loss: 1.7138 - val_accuracy: 0.0000e+00\n",
      "Epoch 137/200\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.9005 - accuracy: 0.1975 - val_loss: 1.6421 - val_accuracy: 0.0247\n",
      "Epoch 138/200\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.9866 - accuracy: 0.2006 - val_loss: 1.6924 - val_accuracy: 0.0123\n",
      "Epoch 139/200\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.8845 - accuracy: 0.2438 - val_loss: 1.6848 - val_accuracy: 0.0247\n",
      "Epoch 140/200\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.8848 - accuracy: 0.2747 - val_loss: 1.7006 - val_accuracy: 0.0000e+00\n",
      "Epoch 141/200\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.8627 - accuracy: 0.2469 - val_loss: 1.6369 - val_accuracy: 0.0247\n",
      "Epoch 142/200\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.9542 - accuracy: 0.2346 - val_loss: 1.7159 - val_accuracy: 0.0000e+00\n",
      "Epoch 143/200\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.8653 - accuracy: 0.2407 - val_loss: 1.6685 - val_accuracy: 0.0247\n",
      "Epoch 144/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.8922 - accuracy: 0.2593 - val_loss: 1.7353 - val_accuracy: 0.0123\n",
      "Epoch 145/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.8711 - accuracy: 0.2315 - val_loss: 1.6400 - val_accuracy: 0.0370\n",
      "Epoch 146/200\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 1.2075 - accuracy: 0.1636 - val_loss: 1.9827 - val_accuracy: 0.0123\n",
      "Epoch 147/200\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 1.0860 - accuracy: 0.2006 - val_loss: 1.8134 - val_accuracy: 0.0123\n",
      "Epoch 148/200\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 1.1296 - accuracy: 0.2037 - val_loss: 2.0193 - val_accuracy: 0.0123\n",
      "Epoch 149/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.9344 - accuracy: 0.2407 - val_loss: 1.6419 - val_accuracy: 0.0247\n",
      "Epoch 150/200\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 1.1140 - accuracy: 0.2037 - val_loss: 1.7492 - val_accuracy: 0.0123\n",
      "Epoch 151/200\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.9850 - accuracy: 0.2160 - val_loss: 1.7388 - val_accuracy: 0.0123\n",
      "Epoch 152/200\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.9382 - accuracy: 0.2438 - val_loss: 1.8141 - val_accuracy: 0.0123\n",
      "Epoch 153/200\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.8727 - accuracy: 0.2346 - val_loss: 1.7433 - val_accuracy: 0.0123\n",
      "Epoch 154/200\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.8875 - accuracy: 0.2562 - val_loss: 1.8845 - val_accuracy: 0.0123\n",
      "Epoch 155/200\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.8421 - accuracy: 0.2531 - val_loss: 1.8074 - val_accuracy: 0.0000e+00\n",
      "Epoch 156/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.8516 - accuracy: 0.2654 - val_loss: 1.8833 - val_accuracy: 0.0247\n",
      "Epoch 157/200\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.8467 - accuracy: 0.2438 - val_loss: 1.7793 - val_accuracy: 0.0000e+00\n",
      "Epoch 158/200\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.8739 - accuracy: 0.2685 - val_loss: 1.9065 - val_accuracy: 0.0000e+00\n",
      "Epoch 159/200\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.8159 - accuracy: 0.2531 - val_loss: 1.8060 - val_accuracy: 0.0000e+00\n",
      "Epoch 160/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.8414 - accuracy: 0.2685 - val_loss: 1.9231 - val_accuracy: 0.0000e+00\n",
      "Epoch 161/200\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.8195 - accuracy: 0.2191 - val_loss: 1.7922 - val_accuracy: 0.0247\n",
      "Epoch 162/200\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.9472 - accuracy: 0.2346 - val_loss: 1.8802 - val_accuracy: 0.0000e+00\n",
      "Epoch 163/200\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.8256 - accuracy: 0.2562 - val_loss: 1.8796 - val_accuracy: 0.0247\n",
      "Epoch 164/200\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.8146 - accuracy: 0.2870 - val_loss: 1.9135 - val_accuracy: 0.0000e+00\n",
      "Epoch 165/200\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.7799 - accuracy: 0.2778 - val_loss: 1.8507 - val_accuracy: 0.0247\n",
      "Epoch 166/200\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.8069 - accuracy: 0.2932 - val_loss: 1.9491 - val_accuracy: 0.0000e+00\n",
      "Epoch 167/200\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.7724 - accuracy: 0.2685 - val_loss: 1.8017 - val_accuracy: 0.0247\n",
      "Epoch 168/200\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.8494 - accuracy: 0.2870 - val_loss: 1.9441 - val_accuracy: 0.0000e+00\n",
      "Epoch 169/200\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.7801 - accuracy: 0.2500 - val_loss: 1.8326 - val_accuracy: 0.0000e+00\n",
      "Epoch 170/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.8321 - accuracy: 0.2716 - val_loss: 1.9499 - val_accuracy: 0.0000e+00\n",
      "Epoch 171/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.7631 - accuracy: 0.2685 - val_loss: 1.8360 - val_accuracy: 0.0123\n",
      "Epoch 172/200\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.7954 - accuracy: 0.2932 - val_loss: 1.9722 - val_accuracy: 0.0000e+00\n",
      "Epoch 173/200\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.7516 - accuracy: 0.2778 - val_loss: 1.8142 - val_accuracy: 0.0123\n",
      "Epoch 174/200\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.8304 - accuracy: 0.2809 - val_loss: 1.9710 - val_accuracy: 0.0000e+00\n",
      "Epoch 175/200\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.7502 - accuracy: 0.2685 - val_loss: 1.8436 - val_accuracy: 0.0123\n",
      "Epoch 176/200\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.7779 - accuracy: 0.3056 - val_loss: 1.8659 - val_accuracy: 0.0000e+00\n",
      "Epoch 177/200\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.7354 - accuracy: 0.2809 - val_loss: 1.8513 - val_accuracy: 0.0247\n",
      "Epoch 178/200\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.8260 - accuracy: 0.2901 - val_loss: 1.7811 - val_accuracy: 0.0000e+00\n",
      "Epoch 179/200\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.7759 - accuracy: 0.2809 - val_loss: 1.6630 - val_accuracy: 0.0494\n",
      "Epoch 180/200\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.8439 - accuracy: 0.2685 - val_loss: 1.7715 - val_accuracy: 0.0123\n",
      "Epoch 181/200\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.8991 - accuracy: 0.2253 - val_loss: 1.6887 - val_accuracy: 0.0494\n",
      "Epoch 182/200\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.9492 - accuracy: 0.2377 - val_loss: 1.8303 - val_accuracy: 0.0247\n",
      "Epoch 183/200\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.8603 - accuracy: 0.2500 - val_loss: 1.7505 - val_accuracy: 0.0123\n",
      "Epoch 184/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.8500 - accuracy: 0.2778 - val_loss: 1.8964 - val_accuracy: 0.0000e+00\n",
      "Epoch 185/200\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.7616 - accuracy: 0.2901 - val_loss: 1.8570 - val_accuracy: 0.0000e+00\n",
      "Epoch 186/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.7324 - accuracy: 0.3056 - val_loss: 1.8929 - val_accuracy: 0.0000e+00\n",
      "Epoch 187/200\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.7358 - accuracy: 0.2932 - val_loss: 1.8593 - val_accuracy: 0.0000e+00\n",
      "Epoch 188/200\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.7257 - accuracy: 0.2901 - val_loss: 1.9007 - val_accuracy: 0.0000e+00\n",
      "Epoch 189/200\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.7402 - accuracy: 0.2840 - val_loss: 1.8434 - val_accuracy: 0.0000e+00\n",
      "Epoch 190/200\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.7317 - accuracy: 0.3148 - val_loss: 1.9214 - val_accuracy: 0.0000e+00\n",
      "Epoch 191/200\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.7139 - accuracy: 0.2809 - val_loss: 1.8578 - val_accuracy: 0.0000e+00\n",
      "Epoch 192/200\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.7354 - accuracy: 0.3086 - val_loss: 1.9323 - val_accuracy: 0.0000e+00\n",
      "Epoch 193/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.7055 - accuracy: 0.2778 - val_loss: 1.8612 - val_accuracy: 0.0000e+00\n",
      "Epoch 194/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.7389 - accuracy: 0.3117 - val_loss: 1.9499 - val_accuracy: 0.0000e+00\n",
      "Epoch 195/200\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.7133 - accuracy: 0.2747 - val_loss: 1.8075 - val_accuracy: 0.0000e+00\n",
      "Epoch 196/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.8110 - accuracy: 0.2716 - val_loss: 1.9403 - val_accuracy: 0.0000e+00\n",
      "Epoch 197/200\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.7098 - accuracy: 0.2747 - val_loss: 1.8989 - val_accuracy: 0.0000e+00\n",
      "Epoch 198/200\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.7283 - accuracy: 0.3056 - val_loss: 1.9502 - val_accuracy: 0.0000e+00\n",
      "Epoch 199/200\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.6871 - accuracy: 0.2932 - val_loss: 1.8969 - val_accuracy: 0.0000e+00\n",
      "Epoch 200/200\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.7324 - accuracy: 0.3117 - val_loss: 1.9758 - val_accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "#from preprocessing import num_encoder_tokens, num_decoder_tokens, decoder_target_data, encoder_input_data, decoder_input_data, decoder_target_data, max_encoder_seq_length, max_decoder_seq_length\n",
    "\n",
    "from tensorflow import keras\n",
    "# Add Dense to the imported layers\n",
    "from keras.layers import Input, LSTM, Dense, Masking\n",
    "from keras.models import Model\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "# Choose dimensionality\n",
    "dimensionality = 256\n",
    "\n",
    "# Choose the batch size\n",
    "# and number of epochs:\n",
    "batch_size = 20\n",
    "epochs = 200\n",
    "\n",
    "# Encoder training setup\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "\n",
    "encoder_lstm = LSTM(dimensionality, return_state=True)\n",
    "encoder_outputs, state_hidden, state_cell = encoder_lstm(encoder_inputs)\n",
    "encoder_states = [state_hidden, state_cell]\n",
    "\n",
    "# Decoder training setup:\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "decoder_lstm = LSTM(dimensionality, return_sequences=True, return_state=True)\n",
    "decoder_outputs, decoder_state_hidden, decoder_state_cell = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Building the training model:\n",
    "training_model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Compile the model:\n",
    "training_model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'], sample_weight_mode='temporal')\n",
    "\n",
    "# print(\"Training the model:\\n\")\n",
    "# Train the model:\n",
    "training_model.fit([encoder_input_data, decoder_input_data], decoder_target_data, batch_size = batch_size, epochs = epochs, validation_split = 0.2)\n",
    "\n",
    "training_model.save('training_model.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac49a615",
   "metadata": {},
   "source": [
    "# test_model1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22e397f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from preprocessing import input_features_dict, target_features_dict, reverse_input_features_dict, reverse_target_features_dict, max_decoder_seq_length, input_docs, target_docs, input_tokens, target_tokens, max_encoder_seq_length\n",
    "#from training_model import decoder_inputs, decoder_lstm, decoder_dense, encoder_input_data, num_decoder_tokens, num_encoder_tokens\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "from keras.models import Model, load_model\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "training_model = load_model('training_model.h5')\n",
    "###### because we're working with a saved model\n",
    "encoder_inputs = training_model.input[0]\n",
    "encoder_outputs, state_h_enc, state_c_enc = training_model.layers[2].output\n",
    "encoder_states = [state_h_enc, state_c_enc]\n",
    "######\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "latent_dim = 256\n",
    "decoder_state_input_hidden = Input(shape=(latent_dim,))\n",
    "decoder_state_input_cell = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_hidden, decoder_state_input_cell]\n",
    "decoder_outputs, state_hidden, state_cell = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_hidden, state_cell]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
    "\n",
    "def decode_sequence(test_input):\n",
    "  # Encode the input as state vectors.\n",
    "  states_value = encoder_model.predict(test_input)\n",
    "\n",
    "  # Generate empty target sequence of length 1.\n",
    "  target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "  # Populate the first token of target sequence with the start token.\n",
    "  target_seq[0, 0, target_features_dict['<START>']] = 1.\n",
    "\n",
    "  # Sampling loop for a batch of sequences\n",
    "  # (to simplify, here we assume a batch of size 1).\n",
    "  decoded_sentence = ''\n",
    "\n",
    "  stop_condition = False\n",
    "  while not stop_condition:\n",
    "    # Run the decoder model to get possible \n",
    "    # output tokens (with probabilities) & states\n",
    "    output_tokens, hidden_state, cell_state = decoder_model.predict(\n",
    "      [target_seq] + states_value)\n",
    "\n",
    "    # Choose token with highest probability\n",
    "    sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "    sampled_token = reverse_target_features_dict[sampled_token_index]\n",
    "    decoded_sentence += \" \" + sampled_token\n",
    "\n",
    "    # Exit condition: either hit max length\n",
    "    # or find stop token.\n",
    "    if (sampled_token == '<END>' or len(decoded_sentence) > max_decoder_seq_length):\n",
    "      stop_condition = True\n",
    "\n",
    "    # Update the target sequence (of length 1).\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "    # Update states\n",
    "    states_value = [hidden_state, cell_state]\n",
    "\n",
    "  return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95147562",
   "metadata": {},
   "source": [
    "# chat.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d7c1445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi, I'm a chatbot trained on dialog. Would you like to chat with me?\n",
      "yes\n",
      " i don't like the that cat catme too\n",
      " i don't like the the that cathaha\n",
      " i don't like the that cat catexit\n",
      "Ok, have a great day!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "#from seq2seq import encoder_model, decoder_model, num_decoder_tokens, num_encoder_tokens, input_features_dict, target_features_dict, reverse_target_features_dict, max_decoder_seq_length, max_encoder_seq_length\n",
    "\n",
    "class ChatBot:\n",
    "  \n",
    "  negative_responses = (\"no\", \"nope\", \"nah\", \"naw\", \"not a chance\", \"sorry\")\n",
    "\n",
    "  exit_commands = (\"quit\", \"pause\", \"exit\", \"goodbye\", \"bye\", \"later\", \"stop\")\n",
    "  \n",
    "  def start_chat(self):\n",
    "    user_response = input(\"Hi, I'm a chatbot trained on dialog. Would you like to chat with me?\\n\")\n",
    "    \n",
    "    if user_response in self.negative_responses:\n",
    "      print(\"Ok, have a great day!\")\n",
    "      return\n",
    "    \n",
    "    self.chat(user_response)\n",
    "  \n",
    "  def chat(self, reply):\n",
    "    while not self.make_exit(reply):\n",
    "      reply = input(self.generate_response(reply))\n",
    "    \n",
    "  def string_to_matrix(self, user_input):\n",
    "    tokens = re.findall(r\"[\\w']+|[^\\s\\w]\", user_input)\n",
    "    user_input_matrix = np.zeros(\n",
    "      (1, max_encoder_seq_length, num_encoder_tokens),\n",
    "      dtype='float32')\n",
    "    for timestep, token in enumerate(tokens):\n",
    "      if token in input_features_dict:\n",
    "        user_input_matrix[0, timestep, input_features_dict[token]] = 1.\n",
    "    return user_input_matrix\n",
    "  \n",
    "  def generate_response(self, user_input):\n",
    "    input_matrix = self.string_to_matrix(user_input)\n",
    "    states_value = encoder_model.predict(input_matrix)\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    target_seq[0, 0, target_features_dict['<START>']] = 1.\n",
    "    \n",
    "    chatbot_response = ''\n",
    "\n",
    "    stop_condition = False\n",
    "    while not stop_condition:\n",
    "      output_tokens, hidden_state, cell_state = decoder_model.predict(\n",
    "        [target_seq] + states_value)\n",
    "      \n",
    "      sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "      sampled_token = reverse_target_features_dict[sampled_token_index]\n",
    "      \n",
    "      chatbot_response += \" \" + sampled_token\n",
    "      \n",
    "      if (sampled_token == '<END>' or len(chatbot_response) > max_decoder_seq_length):\n",
    "        stop_condition = True\n",
    "        \n",
    "      target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "      target_seq[0, 0, sampled_token_index] = 1.\n",
    "      \n",
    "      states_value = [hidden_state, cell_state]\n",
    "      \n",
    "    chatbot_response = chatbot_response.replace(\"<START>\", \"\").replace(\"<END>\", \"\")\n",
    "      \n",
    "    return chatbot_response\n",
    "  \n",
    "  def make_exit(self, reply):\n",
    "    for exit_command in self.exit_commands:\n",
    "      if exit_command in reply:\n",
    "        print(\"Ok, have a great day!\")\n",
    "        return True\n",
    "      \n",
    "    return False\n",
    "  \n",
    "chatty_mcchatface = ChatBot()\n",
    "chatty_mcchatface.start_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027052d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
